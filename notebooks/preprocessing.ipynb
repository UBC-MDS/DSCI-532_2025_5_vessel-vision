{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Combination Process\n",
    "\n",
    "### Step 1: Data Extraction\n",
    "Before we can combine the data, we need to first extract and prepare our raw data files. This is explained in detail in another notebook called **Data Extraction**. In that notebook, we will:\n",
    "\n",
    "1. Download and organize the raw CSV files into the `data/raw` directory.\n",
    "2. Ensure that all the CSV files are correctly formatted and contain the required data.\n",
    "\n",
    "Make sure to run the steps in the **Data Extraction** notebook before proceeding with the next step.\n",
    "\n",
    "### Step 2: Running the Data Combination Script\n",
    "Once the raw data files are available in the `data/raw` folder, you can use the following Python script to combine them into one single CSV file. This script:\n",
    "\n",
    "1. Reads all CSV files in the `data/raw` folder.\n",
    "2. Combines them into a single DataFrame.\n",
    "3. Saves the combined DataFrame as `combined_ais_data.csv` in a new folder called `processed`.\n",
    "\n",
    "If the `processed` folder does not exist, the script will create it automatically.\n",
    "\n",
    "Run the script to generate the combined data that can be used for further analysis or processing.\n",
    "\n",
    "### Example Workflow:\n",
    "1. **Run the Data Extraction notebook** to prepare the `data/raw` folder.\n",
    "2. **Run the Python script** to combine the data into `combined_ais_data.csv` in the `processed` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Script to Combine the Data\n",
    "\n",
    "The Python script provided combines multiple CSV files from the `data/raw` folder into one consolidated CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "✅ Combined CSV file saved at: data\\processed\\combined_ais_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the project root directory name\n",
    "project_root = 'DSCI-532_2025_5_vessel-vision'\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "\n",
    "while project_root not in os.path.basename(current_directory) and current_directory != os.path.dirname(current_directory):\n",
    "    current_directory = os.path.dirname(current_directory)\n",
    "\n",
    "if project_root not in os.path.basename(current_directory):\n",
    "    raise ValueError(f\"Project root '{project_root}' not found.\")\n",
    "\n",
    "\n",
    "os.chdir(current_directory)\n",
    "print(f\"Current working directory: {os.getcwd()}\")  \n",
    "\n",
    "\n",
    "raw_data_folder = os.path.join('data', 'raw')\n",
    "processed_folder_path = os.path.join('data', 'processed') \n",
    "raw_data_path = os.path.join(raw_data_folder, '*.csv')\n",
    "combined_csv_path = os.path.join(processed_folder_path, 'combined_ais_data.csv')\n",
    "\n",
    "# Ensure the raw data folder exists\n",
    "if not os.path.exists(raw_data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data folder not found: {raw_data_folder}\\n\"\n",
    "        \"Please execute the data extraction file first to generate the required data.\"\n",
    "    )\n",
    "\n",
    "# Get all CSV files in the 'data/raw' folder\n",
    "csv_files = glob.glob(raw_data_path)\n",
    "\n",
    "# Check if any CSV files are found\n",
    "if not csv_files:\n",
    "    raise ValueError(\n",
    "        \"No CSV files found in 'data/raw/'.\\n\"\n",
    "        \"Please execute the data extraction file first to generate the required data.\"\n",
    "    )\n",
    "\n",
    "# Ensure the processed folder exists\n",
    "os.makedirs(processed_folder_path, exist_ok=True)\n",
    "\n",
    "# Read and combine all CSV files\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to 'data/processed'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f'✅ Combined CSV file saved at: {combined_csv_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering AIS Data for the West Coast of North America\n",
    "\n",
    "To only include the **West Coast of North America**, you need to filter based on **longitude and latitude**.\n",
    "\n",
    "### Steps:\n",
    "1. **Identify geographic boundaries** of the West Coast of North America.\n",
    "   - Covers **Canada, USA, and Mexico’s Pacific coasts**.\n",
    "   \n",
    "2. **Approximate latitude/longitude range**:\n",
    "   - **Latitude**: 20°N to 60°N  \n",
    "   - **Longitude**: -140°W to -110°W\n",
    "3. **Remove unwanted columns**:\n",
    "   - In the dataset, the following columns were removed: `COG`, `Heading`, `IMO`, `CallSign`, `Length`, `Width`, `Draft`, `Cargo`, `TransceiverClass`. These columns were deemed unnecessary for the analysis, so they were dropped to clean the data and focus on relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azin\\AppData\\Local\\Temp\\ipykernel_43496\\1215247604.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  west_coast_df['Vessel Type Name'] = west_coast_df['VesselType'].apply(\n",
      "C:\\Users\\Azin\\AppData\\Local\\Temp\\ipykernel_43496\\1215247604.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_west_coast_df['Nearest Port'] = filtered_west_coast_df.apply(\n",
      "C:\\Users\\Azin\\AppData\\Local\\Temp\\ipykernel_43496\\1215247604.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_west_coast_df.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered dataset saved at: data\\processed\\ais_west_coast.csv with 579340 records.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Define the project root directory name\n",
    "project_root = 'DSCI-532_2025_5_vessel-vision'\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Find the root directory path\n",
    "while project_root not in os.path.basename(current_directory) and current_directory != os.path.dirname(current_directory):\n",
    "    current_directory = os.path.dirname(current_directory)\n",
    "\n",
    "if project_root not in os.path.basename(current_directory):\n",
    "    raise ValueError(f\"Project root '{project_root}' not found.\")\n",
    "\n",
    "# Change to the root directory\n",
    "os.chdir(current_directory)\n",
    "print(f\"Current working directory: {os.getcwd()}\")  # Debugging statement\n",
    "\n",
    "# Define paths\n",
    "preprocessed_folder = os.path.join('data', 'processed')\n",
    "combined_csv_path = os.path.join(preprocessed_folder, 'combined_ais_data.csv')\n",
    "filtered_csv_path = os.path.join(preprocessed_folder, 'ais_west_coast.csv')\n",
    "\n",
    "# Ensure the preprocessed data folder exists\n",
    "if not os.path.exists(preprocessed_folder):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Preprocessed data folder not found: {preprocessed_folder}\\n\"\n",
    "        \"Please execute the data extraction and combination steps first.\"\n",
    "    )\n",
    "\n",
    "# Load the combined AIS dataset\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Define latitude and longitude boundaries for the West Coast\n",
    "lat_min, lat_max = 20, 60   # Covers from Mexico to Alaska\n",
    "lon_min, lon_max = -140, -110  # Covers the Pacific coast range\n",
    "\n",
    "# Filter data for West Coast\n",
    "west_coast_df = df[\n",
    "    (df['LAT'].between(lat_min, lat_max)) & \n",
    "    (df['LON'].between(lon_min, lon_max))\n",
    "]\n",
    "\n",
    "# Define a sample list of port coordinates (latitude, longitude) and their names\n",
    "ports = [\n",
    "    {\"port\": \"Port of Los Angeles\", \"lat\": 33.74, \"lon\": -118.26},\n",
    "    {\"port\": \"Port of Seattle\", \"lat\": 47.60, \"lon\": -122.33},\n",
    "    {\"port\": \"Port of San Francisco\", \"lat\": 37.78, \"lon\": -122.42},\n",
    "    {\"port\": \"Port of Vancouver\", \"lat\": 49.28, \"lon\": -123.12},\n",
    "    {\"port\": \"Port of Manzanillo\", \"lat\": 19.05, \"lon\": -104.33},  # Mexico\n",
    "    {\"port\": \"Port of Ensenada\", \"lat\": 31.86, \"lon\": -116.60},  # Mexico\n",
    "    {\"port\": \"Port of Mazatlán\", \"lat\": 23.25, \"lon\": -106.41},  # Mexico\n",
    "    {\"port\": \"Port of Lázaro Cárdenas\", \"lat\": 18.12, \"lon\": -102.18},  # Mexico\n",
    "    {\"port\": \"Port of Acapulco\", \"lat\": 16.86, \"lon\": -99.88},  # Mexico\n",
    "    {\"port\": \"Port of Long Beach\", \"lat\": 33.75, \"lon\": -118.20},  # U.S.\n",
    "    {\"port\": \"Port of Oakland\", \"lat\": 37.80, \"lon\": -122.27},  # U.S.\n",
    "    {\"port\": \"Port of San Diego\", \"lat\": 32.72, \"lon\": -117.17},  # U.S.\n",
    "    {\"port\": \"Port of Tacoma\", \"lat\": 47.26, \"lon\": -122.43},  # U.S.\n",
    "]\n",
    "\n",
    "# Function to calculate the distance between two points using the Haversine formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Earth's radius in kilometers\n",
    "    radius = 6371.0\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    return radius * c\n",
    "\n",
    "# Function to find the nearest port based on latitude and longitude\n",
    "def find_nearest_port(lat, lon, ports):\n",
    "    nearest_port = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for port in ports:\n",
    "        port_coords = (port[\"lat\"], port[\"lon\"])\n",
    "        distance = haversine(lat, lon, port_coords[0], port_coords[1])  # Distance in kilometers\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_port = port[\"port\"]\n",
    "    \n",
    "    return nearest_port\n",
    "\n",
    "# Add vessel type name column\n",
    "west_coast_df['Vessel Type Name'] = west_coast_df['VesselType'].apply(\n",
    "    lambda x: 'Passenger' if 60 <= x <= 69 else ('Cargo' if 70 <= x <= 79 else None)\n",
    ")\n",
    "\n",
    "# Filter only Passenger and Cargo vessels\n",
    "filtered_west_coast_df = west_coast_df[west_coast_df['Vessel Type Name'].notna()]\n",
    "\n",
    "# Find the nearest port for each vessel based on latitude and longitude\n",
    "filtered_west_coast_df['Nearest Port'] = filtered_west_coast_df.apply(\n",
    "    lambda row: find_nearest_port(row['LAT'], row['LON'], ports), axis=1\n",
    ")\n",
    "\n",
    "# Remove specified columns\n",
    "columns_to_remove = ['COG', 'Heading', 'IMO', 'CallSign', 'Length', 'Width', 'Draft', 'Cargo', 'TransceiverClass']\n",
    "filtered_west_coast_df.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "# Save the filtered dataset\n",
    "filtered_west_coast_df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "print(f'✅ Filtered dataset saved at: {filtered_csv_path} with {len(filtered_west_coast_df)} records.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split filtered data into multiple CSV files\n",
    "This script splits a large dataset (ais_west_coast.csv) into smaller, approximately 50MB-sized CSV files and stores them in a split-data folder. Before processing the data, the script ensures that it is running from the root directory of the project (DSCI-532_2025_5_vessel-vision). The dataset is read in chunks of 100,000 rows at a time, and the size of each chunk is calculated to ensure it doesn’t exceed 50MB. Once a chunk reaches the target size, it is written to a CSV file with sequential names like ais_chunk_0.csv, ais_chunk_1.csv, etc. If there is leftover data after reading the entire dataset, it is saved to a final chunk file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to project root: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "Removed existing folder: data/split-data/\n",
      "Created new folder: data/split-data/\n",
      "Written ais_chunk_0_part1.csv\n",
      "Written ais_chunk_1.csv\n",
      "Data splitting completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define parameters\n",
    "data_path = \"data/processed/ais_west_coast.csv\"\n",
    "output_folder = \"data/split-data/\"\n",
    "project_root = \"DSCI-532_2025_5_vessel-vision\"\n",
    "chunk_size_in_mb = 100  # Target chunk size in MB (maximum size allowed by GitHub)\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the correct project root directory\n",
    "root_directory = r\"c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\"\n",
    "\n",
    "# Check if we are in the root directory\n",
    "if current_directory != root_directory:\n",
    "    # If not in root, navigate to the root directory\n",
    "    if os.path.exists(root_directory):\n",
    "        os.chdir(root_directory)\n",
    "        print(f\"Changed directory to project root: {root_directory}\")\n",
    "    else:\n",
    "        print(f\"Error: The root directory '{root_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Already in the root directory: {current_directory}\")\n",
    "\n",
    "# Remove the split-data folder if it exists and recreate it\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)  # Remove the existing folder\n",
    "    print(f\"Removed existing folder: {output_folder}\")\n",
    "\n",
    "# Create the split-data folder again\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"Created new folder: {output_folder}\")\n",
    "\n",
    "# Get the size of the file in bytes\n",
    "file_size = os.path.getsize(data_path)  # In bytes\n",
    "target_chunk_size = chunk_size_in_mb * 1024 * 1024  # Convert to bytes\n",
    "\n",
    "# Read the dataset in chunks\n",
    "chunk_index = 0\n",
    "for chunk in pd.read_csv(data_path, chunksize=1000000):  # Read in manageable chunks\n",
    "    # Calculate approximate size of the chunk (in bytes)\n",
    "    chunk_size = chunk.memory_usage(deep=True).sum()\n",
    "\n",
    "    # If the chunk size exceeds the target size, split the chunk\n",
    "    while chunk_size > target_chunk_size:\n",
    "        # Split into smaller chunks (split by rows)\n",
    "        half_chunk = chunk.iloc[:len(chunk) // 2]\n",
    "        chunk = chunk.iloc[len(chunk) // 2:]\n",
    "\n",
    "        # Save the first half\n",
    "        half_chunk_filename = f\"ais_chunk_{chunk_index}_part1.csv\"\n",
    "        half_chunk.to_csv(os.path.join(output_folder, half_chunk_filename), index=False)\n",
    "        print(f\"Written {half_chunk_filename}\")\n",
    "        chunk_index += 1\n",
    "        \n",
    "        # Recalculate chunk size for the remaining part\n",
    "        chunk_size = chunk.memory_usage(deep=True).sum()\n",
    "\n",
    "    # Save the remaining chunk\n",
    "    chunk_filename = f\"ais_chunk_{chunk_index}.csv\"\n",
    "    chunk.to_csv(os.path.join(output_folder, chunk_filename), index=False)\n",
    "    print(f\"Written {chunk_filename}\")\n",
    "    chunk_index += 1\n",
    "\n",
    "print(\"Data splitting completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
