{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Combination Process\n",
    "\n",
    "### Step 1: Data Extraction\n",
    "Before we can combine the data, we need to first extract and prepare our raw data files. This is explained in detail in another notebook called **Data Extraction**. In that notebook, we will:\n",
    "\n",
    "1. Download and organize the raw CSV files into the `data/raw` directory.\n",
    "2. Ensure that all the CSV files are correctly formatted and contain the required data.\n",
    "\n",
    "Make sure to run the steps in the **Data Extraction** notebook before proceeding with the next step.\n",
    "\n",
    "### Step 2: Running the Data Combination Script\n",
    "Once the raw data files are available in the `data/raw` folder, you can use the following Python script to combine them into one single CSV file. This script:\n",
    "\n",
    "1. Reads all CSV files in the `data/raw` folder.\n",
    "2. Combines them into a single DataFrame.\n",
    "3. Saves the combined DataFrame as `combined_ais_data.csv` in a new folder called `processed`.\n",
    "\n",
    "If the `processed` folder does not exist, the script will create it automatically.\n",
    "\n",
    "Run the script to generate the combined data that can be used for further analysis or processing.\n",
    "\n",
    "### Example Workflow:\n",
    "1. **Run the Data Extraction notebook** to prepare the `data/raw` folder.\n",
    "2. **Run the Python script** to combine the data into `combined_ais_data.csv` in the `processed` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Script to Combine the Data\n",
    "\n",
    "The Python script provided combines multiple CSV files from the `data/raw` folder into one consolidated CSV file. Here's a breakdown of how the script works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "✅ Combined CSV file saved at: data\\processed\\combined_ais_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the project root directory name\n",
    "project_root = 'DSCI-532_2025_5_vessel-vision'\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "\n",
    "while project_root not in os.path.basename(current_directory) and current_directory != os.path.dirname(current_directory):\n",
    "    current_directory = os.path.dirname(current_directory)\n",
    "\n",
    "if project_root not in os.path.basename(current_directory):\n",
    "    raise ValueError(f\"Project root '{project_root}' not found.\")\n",
    "\n",
    "\n",
    "os.chdir(current_directory)\n",
    "print(f\"Current working directory: {os.getcwd()}\")  \n",
    "\n",
    "\n",
    "raw_data_folder = os.path.join('data', 'raw')\n",
    "processed_folder_path = os.path.join('data', 'processed') \n",
    "raw_data_path = os.path.join(raw_data_folder, '*.csv')\n",
    "combined_csv_path = os.path.join(processed_folder_path, 'combined_ais_data.csv')\n",
    "\n",
    "# Ensure the raw data folder exists\n",
    "if not os.path.exists(raw_data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data folder not found: {raw_data_folder}\\n\"\n",
    "        \"Please execute the data extraction file first to generate the required data.\"\n",
    "    )\n",
    "\n",
    "# Get all CSV files in the 'data/raw' folder\n",
    "csv_files = glob.glob(raw_data_path)\n",
    "\n",
    "# Check if any CSV files are found\n",
    "if not csv_files:\n",
    "    raise ValueError(\n",
    "        \"No CSV files found in 'data/raw/'.\\n\"\n",
    "        \"Please execute the data extraction file first to generate the required data.\"\n",
    "    )\n",
    "\n",
    "# Ensure the processed folder exists\n",
    "os.makedirs(processed_folder_path, exist_ok=True)\n",
    "\n",
    "# Read and combine all CSV files\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to 'data/processed'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f'✅ Combined CSV file saved at: {combined_csv_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering AIS Data for the West Coast of North America\n",
    "\n",
    "To only include the **West Coast of North America**, you need to filter based on **longitude and latitude**.\n",
    "\n",
    "### Steps:\n",
    "1. **Identify geographic boundaries** of the West Coast of North America.\n",
    "   - Covers **Canada, USA, and Mexico’s Pacific coasts**.\n",
    "   \n",
    "2. **Approximate latitude/longitude range**:\n",
    "   - **Latitude**: 20°N to 60°N  \n",
    "   - **Longitude**: -140°W to -110°W  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "✅ Filtered dataset saved at: data\\processed\\ais_west_coast.csv with 3716130 records.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the project root directory name\n",
    "project_root = 'DSCI-532_2025_5_vessel-vision'\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Find the root directory path\n",
    "while project_root not in os.path.basename(current_directory) and current_directory != os.path.dirname(current_directory):\n",
    "    current_directory = os.path.dirname(current_directory)\n",
    "\n",
    "if project_root not in os.path.basename(current_directory):\n",
    "    raise ValueError(f\"Project root '{project_root}' not found.\")\n",
    "\n",
    "# Change to the root directory\n",
    "os.chdir(current_directory)\n",
    "print(f\"Current working directory: {os.getcwd()}\")  # Debugging statement\n",
    "\n",
    "# Define paths\n",
    "preprocessed_folder = os.path.join('data', 'processed')\n",
    "combined_csv_path = os.path.join(preprocessed_folder, 'combined_ais_data.csv')\n",
    "filtered_csv_path = os.path.join(preprocessed_folder, 'ais_west_coast.csv')\n",
    "\n",
    "# Ensure the preprocessed data folder exists\n",
    "if not os.path.exists(preprocessed_folder):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Preprocessed data folder not found: {preprocessed_folder}\\n\"\n",
    "        \"Please execute the data extraction and combination steps first.\"\n",
    "    )\n",
    "\n",
    "# Load the combined AIS dataset\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Define latitude and longitude boundaries for the West Coast\n",
    "lat_min, lat_max = 20, 60   # Covers from Mexico to Alaska\n",
    "lon_min, lon_max = -140, -110  # Covers the Pacific coast range\n",
    "\n",
    "# Filter data for West Coast\n",
    "west_coast_df = df[\n",
    "    (df['LAT'].between(lat_min, lat_max)) &\n",
    "    (df['LON'].between(lon_min, lon_max))\n",
    "]\n",
    "\n",
    "# Save the filtered dataset\n",
    "west_coast_df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "print(f'✅ Filtered dataset saved at: {filtered_csv_path} with {len(west_coast_df)} records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split filtered data into multiple CSV files\n",
    "This script splits a large dataset (ais_west_coast.csv) into smaller, approximately 50MB-sized CSV files and stores them in a split-data folder. Before processing the data, the script ensures that it is running from the root directory of the project (DSCI-532_2025_5_vessel-vision). The dataset is read in chunks of 100,000 rows at a time, and the size of each chunk is calculated to ensure it doesn’t exceed 50MB. Once a chunk reaches the target size, it is written to a CSV file with sequential names like ais_chunk_0.csv, ais_chunk_1.csv, etc. If there is leftover data after reading the entire dataset, it is saved to a final chunk file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in the root directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "Written final chunk 0 to data/split-data/ais_chunk_0.csv\n",
      "Data splitting completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define parameters\n",
    "data_path = \"data/processed/ais_west_coast.csv\"\n",
    "output_folder = \"data/split-data/\"  # Specify that 'split-data' should be inside the 'data' folder\n",
    "project_root = \"DSCI-532_2025_5_vessel-vision\"\n",
    "chunk_size_in_mb = 50  # Target chunk size in MB\n",
    "\n",
    "# Ensure the script is running from the root directory\n",
    "current_directory = os.getcwd()\n",
    "if os.path.basename(current_directory) != project_root:\n",
    "    root_directory = os.path.abspath(os.path.join(current_directory, '..', project_root))\n",
    "    os.chdir(root_directory)\n",
    "    print(f\"Changed directory to project root: {root_directory}\")\n",
    "else:\n",
    "    print(f\"Already in the root directory: {current_directory}\")\n",
    "\n",
    "# Create the split-data folder inside the 'data' folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get the size of the file in bytes\n",
    "file_size = os.path.getsize(data_path)  # In bytes\n",
    "target_chunk_size = chunk_size_in_mb * 1024 * 1024  # Convert to bytes\n",
    "\n",
    "# Initialize variables\n",
    "chunk_index = 0\n",
    "current_chunk = []\n",
    "\n",
    "# Read the file in chunks based on the file size (not rows)\n",
    "for chunk in pd.read_csv(data_path, chunksize=100000):  # Read in manageable chunks\n",
    "    current_chunk.append(chunk)\n",
    "    current_chunk_size = sum([len(str(row)) for row in chunk.values.flatten()])  # Calculate approximate size of chunk\n",
    "    \n",
    "    # If the current chunk is larger than the target size, write it to a CSV file\n",
    "    if current_chunk_size >= target_chunk_size:\n",
    "        chunk_filename = f\"ais_chunk_{chunk_index}.csv\"\n",
    "        chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "        pd.concat(current_chunk).to_csv(chunk_path, index=False)  # Combine chunk and write\n",
    "        print(f\"Written chunk {chunk_index} to {chunk_path}\")\n",
    "        \n",
    "        # Reset for next chunk\n",
    "        current_chunk = []\n",
    "        chunk_index += 1\n",
    "\n",
    "# If there is any leftover data in the current_chunk list, write it out\n",
    "if current_chunk:\n",
    "    chunk_filename = f\"ais_chunk_{chunk_index}.csv\"\n",
    "    chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "    pd.concat(current_chunk).to_csv(chunk_path, index=False)\n",
    "    print(f\"Written final chunk {chunk_index} to {chunk_path}\")\n",
    "\n",
    "print(\"Data splitting completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vessel-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
