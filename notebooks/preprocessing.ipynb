{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Combination Process\n",
    "\n",
    "### Step 1: Data Extraction\n",
    "Before we can combine the data, we need to first extract and prepare our raw data files. This is explained in detail in another notebook called **Data Extraction**. In that notebook, we will:\n",
    "\n",
    "1. Download and organize the raw CSV files into the `data/raw` directory.\n",
    "2. Ensure that all the CSV files are correctly formatted and contain the required data.\n",
    "\n",
    "Make sure to run the steps in the **Data Extraction** notebook before proceeding with the next step.\n",
    "\n",
    "### Step 2: Running the Data Combination Script\n",
    "Once the raw data files are available in the `data/raw` folder, you can use the following Python script to combine them into one single CSV file. This script:\n",
    "\n",
    "1. Reads all CSV files in the `data/raw` folder.\n",
    "2. Combines them into a single DataFrame.\n",
    "3. Saves the combined DataFrame as `combined_ais_data.csv` in a new folder called `processed`.\n",
    "\n",
    "If the `processed` folder does not exist, the script will create it automatically.\n",
    "\n",
    "Run the script to generate the combined data that can be used for further analysis or processing.\n",
    "\n",
    "### Example Workflow:\n",
    "1. **Run the Data Extraction notebook** to prepare the `data/raw` folder.\n",
    "2. **Run the Python script** to combine the data into `combined_ais_data.csv` in the `processed` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Script to Combine the Data\n",
    "\n",
    "The Python script provided combines multiple CSV files from the `data/raw` folder into one consolidated CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "✅ Combined CSV file saved at: data\\processed\\combined_ais_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the project root directory name\n",
    "project_root = 'DSCI-532_2025_5_vessel-vision'\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "\n",
    "while project_root not in os.path.basename(current_directory) and current_directory != os.path.dirname(current_directory):\n",
    "    current_directory = os.path.dirname(current_directory)\n",
    "\n",
    "if project_root not in os.path.basename(current_directory):\n",
    "    raise ValueError(f\"Project root '{project_root}' not found.\")\n",
    "\n",
    "\n",
    "os.chdir(current_directory)\n",
    "print(f\"Current working directory: {os.getcwd()}\")  \n",
    "\n",
    "\n",
    "raw_data_folder = os.path.join('data', 'raw')\n",
    "processed_folder_path = os.path.join('data', 'processed') \n",
    "raw_data_path = os.path.join(raw_data_folder, '*.csv')\n",
    "combined_csv_path = os.path.join(processed_folder_path, 'combined_ais_data.csv')\n",
    "\n",
    "# Ensure the raw data folder exists\n",
    "if not os.path.exists(raw_data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data folder not found: {raw_data_folder}\\n\"\n",
    "        \"Please execute the data extraction file first to generate the required data.\"\n",
    "    )\n",
    "\n",
    "# Get all CSV files in the 'data/raw' folder\n",
    "csv_files = glob.glob(raw_data_path)\n",
    "\n",
    "# Check if any CSV files are found\n",
    "if not csv_files:\n",
    "    raise ValueError(\n",
    "        \"No CSV files found in 'data/raw/'.\\n\"\n",
    "        \"Please execute the data extraction file first to generate the required data.\"\n",
    "    )\n",
    "\n",
    "# Ensure the processed folder exists\n",
    "os.makedirs(processed_folder_path, exist_ok=True)\n",
    "\n",
    "# Read and combine all CSV files\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to 'data/processed'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f'✅ Combined CSV file saved at: {combined_csv_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering AIS Data for the West Coast of North America\n",
    "\n",
    "To only include the **West Coast of North America**, you need to filter based on **longitude and latitude**.\n",
    "\n",
    "### Steps:\n",
    "1. **Identify geographic boundaries** of the West Coast of North America.\n",
    "   - Covers **Canada, USA, and Mexico’s Pacific coasts**.\n",
    "   \n",
    "2. **Approximate latitude/longitude range**:\n",
    "   - **Latitude**: 20°N to 60°N  \n",
    "   - **Longitude**: -140°W to -110°W  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azin\\AppData\\Local\\Temp\\ipykernel_6600\\557139887.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  west_coast_df['Vessel Type Name'] = west_coast_df['VesselType'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered dataset saved at: data\\processed\\ais_west_coast.csv with 579340 records.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the project root directory name\n",
    "project_root = 'DSCI-532_2025_5_vessel-vision'\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Find the root directory path\n",
    "while project_root not in os.path.basename(current_directory) and current_directory != os.path.dirname(current_directory):\n",
    "    current_directory = os.path.dirname(current_directory)\n",
    "\n",
    "if project_root not in os.path.basename(current_directory):\n",
    "    raise ValueError(f\"Project root '{project_root}' not found.\")\n",
    "\n",
    "# Change to the root directory\n",
    "os.chdir(current_directory)\n",
    "print(f\"Current working directory: {os.getcwd()}\")  # Debugging statement\n",
    "\n",
    "# Define paths\n",
    "preprocessed_folder = os.path.join('data', 'processed')\n",
    "combined_csv_path = os.path.join(preprocessed_folder, 'combined_ais_data.csv')\n",
    "filtered_csv_path = os.path.join(preprocessed_folder, 'ais_west_coast.csv')\n",
    "\n",
    "# Ensure the preprocessed data folder exists\n",
    "if not os.path.exists(preprocessed_folder):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Preprocessed data folder not found: {preprocessed_folder}\\n\"\n",
    "        \"Please execute the data extraction and combination steps first.\"\n",
    "    )\n",
    "\n",
    "# Load the combined AIS dataset\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Define latitude and longitude boundaries for the West Coast\n",
    "lat_min, lat_max = 20, 60   # Covers from Mexico to Alaska\n",
    "lon_min, lon_max = -140, -110  # Covers the Pacific coast range\n",
    "\n",
    "# Filter data for West Coast\n",
    "west_coast_df = df[\n",
    "    (df['LAT'].between(lat_min, lat_max)) &\n",
    "    (df['LON'].between(lon_min, lon_max))\n",
    "]\n",
    "\n",
    "# Add vessel type name column\n",
    "west_coast_df['Vessel Type Name'] = west_coast_df['VesselType'].apply(\n",
    "    lambda x: 'Passenger' if 60 <= x <= 69 else ('Cargo' if 70 <= x <= 79 else None)\n",
    ")\n",
    "\n",
    "# Filter only Passenger and Cargo vessels\n",
    "filtered_west_coast_df = west_coast_df[west_coast_df['Vessel Type Name'].notna()]\n",
    "\n",
    "# Save the filtered dataset\n",
    "filtered_west_coast_df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "print(f'✅ Filtered dataset saved at: {filtered_csv_path} with {len(filtered_west_coast_df)} records.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split filtered data into multiple CSV files\n",
    "This script splits a large dataset (ais_west_coast.csv) into smaller, approximately 50MB-sized CSV files and stores them in a split-data folder. Before processing the data, the script ensures that it is running from the root directory of the project (DSCI-532_2025_5_vessel-vision). The dataset is read in chunks of 100,000 rows at a time, and the size of each chunk is calculated to ensure it doesn’t exceed 50MB. Once a chunk reaches the target size, it is written to a CSV file with sequential names like ais_chunk_0.csv, ais_chunk_1.csv, etc. If there is leftover data after reading the entire dataset, it is saved to a final chunk file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to project root: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\n",
      "Created new folder: data/split-data/\n",
      "Written ais_chunk_0_part1.csv\n",
      "Written ais_chunk_1_part1.csv\n",
      "Written ais_chunk_2.csv\n",
      "Data splitting completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define parameters\n",
    "data_path = \"data/processed/ais_west_coast.csv\"\n",
    "output_folder = \"data/split-data/\"\n",
    "project_root = \"DSCI-532_2025_5_vessel-vision\"\n",
    "chunk_size_in_mb = 100  # Target chunk size in MB (maximum size allowed by GitHub)\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the correct project root directory\n",
    "root_directory = r\"c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\"\n",
    "\n",
    "# Check if we are in the root directory\n",
    "if current_directory != root_directory:\n",
    "    # If not in root, navigate to the root directory\n",
    "    if os.path.exists(root_directory):\n",
    "        os.chdir(root_directory)\n",
    "        print(f\"Changed directory to project root: {root_directory}\")\n",
    "    else:\n",
    "        print(f\"Error: The root directory '{root_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Already in the root directory: {current_directory}\")\n",
    "\n",
    "# Remove the split-data folder if it exists and recreate it\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)  # Remove the existing folder\n",
    "    print(f\"Removed existing folder: {output_folder}\")\n",
    "\n",
    "# Create the split-data folder again\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"Created new folder: {output_folder}\")\n",
    "\n",
    "# Get the size of the file in bytes\n",
    "file_size = os.path.getsize(data_path)  # In bytes\n",
    "target_chunk_size = chunk_size_in_mb * 1024 * 1024  # Convert to bytes\n",
    "\n",
    "# Read the dataset in chunks\n",
    "chunk_index = 0\n",
    "for chunk in pd.read_csv(data_path, chunksize=1000000):  # Read in manageable chunks\n",
    "    # Calculate approximate size of the chunk (in bytes)\n",
    "    chunk_size = chunk.memory_usage(deep=True).sum()\n",
    "\n",
    "    # If the chunk size exceeds the target size, split the chunk\n",
    "    while chunk_size > target_chunk_size:\n",
    "        # Split into smaller chunks (split by rows)\n",
    "        half_chunk = chunk.iloc[:len(chunk) // 2]\n",
    "        chunk = chunk.iloc[len(chunk) // 2:]\n",
    "\n",
    "        # Save the first half\n",
    "        half_chunk_filename = f\"ais_chunk_{chunk_index}_part1.csv\"\n",
    "        half_chunk.to_csv(os.path.join(output_folder, half_chunk_filename), index=False)\n",
    "        print(f\"Written {half_chunk_filename}\")\n",
    "        chunk_index += 1\n",
    "        \n",
    "        # Recalculate chunk size for the remaining part\n",
    "        chunk_size = chunk.memory_usage(deep=True).sum()\n",
    "\n",
    "    # Save the remaining chunk\n",
    "    chunk_filename = f\"ais_chunk_{chunk_index}.csv\"\n",
    "    chunk.to_csv(os.path.join(output_folder, chunk_filename), index=False)\n",
    "    print(f\"Written {chunk_filename}\")\n",
    "    chunk_index += 1\n",
    "\n",
    "print(\"Data splitting completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\\notebooks\n",
      "Project root directory: c:\\Users\\Azin\\Desktop\\Azin files\\Azin's Document\\UBC\\block 5\\532_viz-2\\DSCI-532_2025_5_vessel-vision\\DSCI-532_2025_5_vessel-vision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "# Define the project root (modify this as needed for your project)\n",
    "project_root = \"DSCI-532_2025_5_vessel-vision\"\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, '..', project_root))\n",
    "\n",
    "# Print the root directory (absolute path)\n",
    "print(f\"Project root directory: {root_directory}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vessel-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
